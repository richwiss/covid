{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-time directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = '.'\n",
    "population_loc = f'{base_loc}/resources'\n",
    "\n",
    "# jhu_loc is the root directory of the JHU data repository\n",
    "jhu_loc = f'{base_loc}/COVID-19'\n",
    "csse_loc = f'{jhu_loc}/csse_covid_19_data/csse_covid_19_daily_reports'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load population and region data\n",
    "\n",
    "Note: Regions only supported for Pennsylvania and New York in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_population_data():\n",
    "    \"\"\" load population and region data for counties in PA and other supported states \"\"\"\n",
    "    df = pd.read_csv(f'{population_loc}/county-populations.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JHU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\" read all the CSV files into a single data frame \"\"\"\n",
    "    csv_files = [fname for fname in os.listdir(csse_loc) if fname.endswith('.csv')]\n",
    "    data = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(f'{csse_loc}/{csv_file}', dtype={\"FIPS\": str})\n",
    "        # At some point JHU renamed some columns\n",
    "        df = df.rename(columns={'Province/State': 'Province_State', \n",
    "                                'Last Update': 'Last_Update',\n",
    "                                'Country/Region': 'Country_Region',\n",
    "                                'Latitude': 'Lat',\n",
    "                                'Longitude': 'Long_'})\n",
    "\n",
    "        data.append(df)\n",
    "\n",
    "    df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "    # Remove unneeded columns\n",
    "    df = df.drop(columns=['Lat', 'Long_', 'Combined_Key', 'FIPS'])\n",
    "    \n",
    "    # In later data, \"Active\" = \"Confirmed\" - \"Deaths\". If \"Active\" == 0, compute it\n",
    "    \n",
    "\n",
    "    # Standardize all dates to noon\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "    df.Last_Update = df.Last_Update.dt.strftime('%m/%d/%Y')\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "\n",
    "    # Fix \"active\" column when it is set to 0 prior to it being reported\n",
    "    def active_fn(row):\n",
    "        if row.Active == 0:\n",
    "            return row.Confirmed - row.Recovered - row.Deaths\n",
    "        else:\n",
    "            return row.Active\n",
    "\n",
    "    df = df.assign(Active=df.apply(active_fn, axis=1)) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_data():\n",
    "    \"\"\" read the series data in the JHU directory \"\"\"\n",
    "    series_loc = f'{jhu_loc}/csse_covid_19_data/csse_covid_19_time_series'\n",
    "    df = pd.read_csv(f'{series_loc}/time_series_covid19_confirmed_US.csv', dtype={\"FIPS\": str})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and filter all data for just one state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_state(df, state, popdf):\n",
    "    \"\"\" merge individual counties in a state into a single row \"\"\"\n",
    "    statepop_dict = state_populations(popdf)\n",
    "\n",
    "    merged = pd.DataFrame()\n",
    "    merged['Last_Update'] = df[df.Province_State==state].groupby(df.Last_Update)['Last_Update'].unique()    \n",
    "    merged['Admin2'] = 'All'\n",
    "    merged['Province_State'] = state\n",
    "    merged['Country_Region'] = df.Country_Region.unique()[0]\n",
    "\n",
    "    merged['Deaths'] = df[df.Province_State==state].groupby(df.Last_Update)['Deaths'].sum()\n",
    "    merged['Confirmed'] = df[df.Province_State==state].groupby(df.Last_Update)['Confirmed'].sum()\n",
    "    merged['Recovered'] = df[df.Province_State==state].groupby(df.Last_Update)['Recovered'].sum()\n",
    "    merged['Active'] = df[df.Province_State==state].groupby(df.Last_Update)['Active'].sum()\n",
    "    merged['Population'] = statepop_dict[state]\n",
    "\n",
    "    merged['Last_Update'] = merged.index\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_data(state, df):\n",
    "    state_matches = df[(df.Province_State==state)]\n",
    "    state_matches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return pd.DataFrame(state_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge for just one region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_region(df, region, popdf):\n",
    "    \"\"\" for PA, merge the data into regions \"\"\"\n",
    "    regionpop_dict = region_populations(popdf)\n",
    "    \n",
    "    merged = pd.DataFrame()\n",
    "    merged['Last_Update'] = df[df.Region==region].groupby(df.Last_Update)['Last_Update'].unique()    \n",
    "    merged['Admin2'] = region\n",
    "    merged['Province_State'] = df.Province_State.unique()[0]\n",
    "    merged['Country_Region'] = df.Country_Region.unique()[0]\n",
    "\n",
    "    merged['Deaths'] = df[df.Region==region].groupby(df.Last_Update)['Deaths'].sum()\n",
    "    merged['Confirmed'] = df[df.Region==region].groupby(df.Last_Update)['Confirmed'].sum()\n",
    "    merged['Recovered'] = df[df.Region==region].groupby(df.Last_Update)['Recovered'].sum()\n",
    "    merged['Active'] = df[df.Region==region].groupby(df.Last_Update)['Active'].sum()\n",
    "\n",
    "    merged['Population'] = regionpop_dict[region]\n",
    "    \n",
    "    merged['Last_Update'] = merged.index\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter all data for just one county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_data(state, county, df):\n",
    "    county_matches = df[(df.Province_State==state) & (df.Admin2==county)]\n",
    "    county_matches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return pd.DataFrame(county_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the dataframe with region information, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def annotate_regions(df, popdf):\n",
    "    \"\"\" For counties in Pennsylvania, annotate the dataframe with the region \"\"\"\n",
    "    \n",
    "    def annotator(row):\n",
    "        poprow = popdf[(popdf.State==row.Province_State)&(popdf.County==row.Admin2)]\n",
    "        if len(poprow) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return poprow.Region.values[0]\n",
    "\n",
    "    df['Region'] = df.apply(annotator, axis=1)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_regions(df, popdf):\n",
    "    \"\"\" Annotate the dataframe with the region, if available (PA and NYC only) \"\"\"\n",
    "\n",
    "    region_map = defaultdict(lambda: np.nan)\n",
    "    for d in popdf.to_dict('records'):\n",
    "        region_map[d['State'], d['County']] = d['Region']\n",
    "    df['Region'] = df[['Province_State', 'Admin2']].apply(lambda x: region_map[x[0], x[1]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the dataframe with populations, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_populations(df, popdf):\n",
    "    \"\"\" Annotate the dataframe with populations, if available \"\"\"\n",
    "        \n",
    "    pop_map = defaultdict(lambda: np.nan)\n",
    "    for d in popdf.to_dict('records'):\n",
    "        pop_map[d['State'], d['County']] = d['Population']\n",
    "    df['Population'] = df.loc[:,['Province_State','Admin2']].apply(lambda x: pop_map[x[0], x[1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_populations(popdf):\n",
    "    return dict(popdf.groupby(popdf.State)['Population'].sum().items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_populations(popdf):\n",
    "    \"\"\" for PA, calculate the population of each region \"\"\"   \n",
    "    return dict(popdf.groupby(popdf.Region)['Population'].sum().items())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the appropriate locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_locality(all_df, popdf, query_type, query_state=None, query_region=None, query_county=None):\n",
    "    if query_type == 'State':\n",
    "        label = f'{query_state} State'\n",
    "        df = get_state_data(query_state, all_df)\n",
    "        df = merge_state(df, query_state, popdf)\n",
    "    elif query_type == 'Region':\n",
    "        label = f'{query_region} Region, {query_state}'\n",
    "        df = get_state_data(query_state, all_df)\n",
    "        if 'Region' in df.columns:\n",
    "            df['Admin2'] = df['Region']\n",
    "        else:\n",
    "            annotate_regions(df, popdf)\n",
    "        df = merge_region(df, query_region, popdf)\n",
    "    elif query_type == 'County':\n",
    "        label = f'{query_county} County, {query_state}'\n",
    "        df=get_county_data(query_state, query_county, all_df)\n",
    "        annotate_populations(df, popdf)\n",
    "    \n",
    "    return df, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute daily and average new cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cases(df):\n",
    "    \"\"\" given a DataFrame with a .Confirmed field, add a .New_Cases field that\n",
    "    has new cases per day. \"\"\"\n",
    "    df['New_Cases'] = df.Confirmed.subtract(df.Confirmed.shift(1), fill_value=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_new_cases(df, days, centered=False):\n",
    "    \"\"\" this computes day the trailing average in the final day \"\"\"\n",
    "    \"\"\" compute the moving average over {days} days and add as day_avg_{days} to the df \"\"\"\n",
    "    field = f'day_avg_{days}'\n",
    "    df[field] = df.New_Cases.rolling(window=14, min_periods=1, center=centered).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_avg(dates):\n",
    "  refdate = datetime.datetime(2019, 1, 1)\n",
    "  return refdate + sum([date - refdate for date in dates], datetime.timedelta()) / len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily new cases and 7-day moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_case_plot(df, label, days=7, centered=False, output=None):\n",
    "\n",
    "    if centered:\n",
    "        date_field = f'Centered_Date_{days}'\n",
    "    else:\n",
    "        date_field='Last_Update'\n",
    "    average_new_cases(df, days, centered=centered)\n",
    "\n",
    "        \n",
    "    g = sns.lineplot(df['Last_Update'], df['New_Cases'], label=\"Daily new cases\")\n",
    "    sns.lineplot(df[date_field], df[f'day_avg_{days}'], ax=g, label=f\"{days} day moving average\")\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"New Cases\", title=f\"New Cases Per Day\\n{label}\")\n",
    "    leg = g.legend(loc='upper left', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_new_cases.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow target: 50 new cases over 14 days per 100K people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newcase_sum(df, days, perpop=1):\n",
    "    \"\"\" \n",
    "    compute the sum of {days} days and {days}_sum to the df \n",
    "    if perpop is not 1, calculate the same weighted by the population pop\n",
    "    \"\"\"\n",
    "    field = f'sum_{days}'\n",
    "    df[field] = df.New_Cases.rolling(window=days, min_periods=1).sum()\n",
    "    df[field] *= perpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellow_target(df, label, output=None):\n",
    "    population = set(df.Population).pop()\n",
    "    newcase_sum(df, 14, perpop=100000/population)\n",
    "    target = 50\n",
    "    \n",
    "    g = sns.lineplot(df['Last_Update'], df['sum_14'], label=\"14 day caseload per 100K\")\n",
    "    sns.lineplot(df['Last_Update'], [target]*len(df), label=\"Yellow Target\", ax=g)\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"14 days cases per 100K\", title=f\"Progress towards yellow target\\n{label}\")\n",
    "    leg = g.legend(loc='lower right', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_yellow_target.png')\n",
    "        plt.savefig(output, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days trending downward in 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_xticks(labels, num=5):\n",
    "    \"\"\"\n",
    "    For some reason I can't limit the number of xticks so here I'm\n",
    "    just doing it myself by erasing the text of the xticks I don't want\n",
    "    \"\"\"\n",
    "        \n",
    "    target_ticks = set([0, len(labels)-1])\n",
    "    for i in range(1, num-1):\n",
    "        pos= int(round(len(labels)/(num-1)*i,0))\n",
    "        target_ticks.add(pos)\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        if i not in target_ticks:\n",
    "            labels[i].set_text(\"\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(period):\n",
    "    if len(period) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        m, b = np.polyfit(np.arange(len(period)), period, 1)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend(df, days):\n",
    "    \"\"\" \n",
    "    compute the trendline for the past {days} days as slope_{days} and\n",
    "    the number of days within those {days} that the trend is worsening \n",
    "    (positive) or improving (negative) as {days}_trend\n",
    "    \"\"\"\n",
    "    slopes = []\n",
    "    trends = []\n",
    "\n",
    "    # Get the slope of the trend line for the past {days} days.\n",
    "    sfield=f'slope_{days}'\n",
    "    df[sfield] = df.New_Cases.rolling(window=days, min_periods=1).apply(fit)\n",
    "\n",
    "    # Get the number of times the slope was positive in last {days} days.\n",
    "    field = f'trend_{days}'\n",
    "    df[field] = df[sfield].rolling(window=14, min_periods=14).apply(lambda x: (x>0).sum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending(df, label, days=14, output=None):\n",
    "    df = trend(df, 14)\n",
    "\n",
    "    tfield = f'trend_{days}'\n",
    "    sfield = f'slope_{days}'\n",
    "    \n",
    "    formatted_dates = df['Last_Update'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    g=sns.barplot(formatted_dates, df[tfield], label=\"increasing trends\", color='red')\n",
    "    sns.barplot(formatted_dates, df[tfield]-14, label=\"decreasing trends\", color='green')\n",
    "    t = g.twinx()\n",
    "    \n",
    "    sns.lineplot(np.arange(len(df)), df[sfield], color=\"black\", label=\"14-day slope\", ax=t)\n",
    "    #slopes = np.where(df['trend_14'].isnull(), 0, df['slope_14'])\n",
    "    #sns.lineplot(np.arange(len(df)), slopes, color=\"black\", label=\"14-day slope\", ax=t)\n",
    "\n",
    "    labels = limit_xticks(g.get_xticklabels())\n",
    "    g.set_xticklabels(labels,rotation=90)\n",
    "\n",
    "    g.set_ylim(-14,14)\n",
    "    title=f\"Number of days in the past two weeks with a positive or negative trend\\n{label}\"\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Number of days\", title=title)\n",
    "    t.set(ylabel=\"slope of 14-day trend\")\n",
    "    slope_lim = max(abs(df[df[sfield].notna()][sfield]))*1.1\n",
    "    t.set_ylim(-slope_lim,slope_lim)\n",
    "    leg = t.legend(loc='lower left', frameon=False)\n",
    "\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_trend.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues\n",
    "\n",
    "* States allocate cases to \"Unassigned\" if county is unknown\n",
    "* \"Out of CO\", \"Out of GA\", \"Out of MI\", \"Out of OK\", \"Out of TN\" is listed as a county\n",
    "* Dukes, MA and Nantucket, MA -> \"Dukes and Nantucket\"\n",
    "* Federal Correctional Institution (FCI), MI; Michigan Department of Corrections (MDOC), MI\n",
    "* Kansas City, MO reported as a standalone county when it actually appears in multiple counties\n",
    "* New York City, NY is reported but counties are Richmond, Queens, New York, Kings and Bronx\n",
    "* Counties in Utah don't align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read county population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "popdf = load_population_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JHU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read JHU daily_reports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not series_data:\n",
    "    all_df = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read JHU time_series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "if series_data:\n",
    "    all_sdf = get_series_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output all graphs for specified state, region or county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_at_date(df, date):\n",
    "    \"\"\"\n",
    "    Start the time series on this date\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_at_zero_series(df):\n",
    "    \"\"\"\n",
    "    Start the data the day before the first confirmed case\n",
    "    Assumes data frame is in time_series format\n",
    "    \"\"\"\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in df.columns if rgx.search(c)]\n",
    "    drops = []\n",
    "    for c in date_cols:\n",
    "        sm = df[c].sum()\n",
    "        if sm == 0:\n",
    "            drops.append(c)\n",
    "        elif sm > 0:\n",
    "            break\n",
    "    if len(drops) < len(df.columns):\n",
    "        df = df.drop(columns=drops)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_at_zero(df):\n",
    "    \"\"\"\n",
    "    Start the data the day before the first confirmed case\n",
    "    Assumes data frame is in daily_reports format\n",
    "    \"\"\"\n",
    "    non_zeros = df[df.Confirmed > 0]\n",
    "    if len(non_zeros) > 0:\n",
    "        first_non_zero = min(non_zeros.index)\n",
    "    last_zero = max(0, first_non_zero-1)\n",
    "    cdf = pd.DataFrame(df.iloc[last_zero:])\n",
    "    cdf = cdf.reset_index(drop=True)\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_helper(df, label, output, output_directory):\n",
    "    df = df.sort_values(by='Last_Update', ignore_index=True)\n",
    "    if output == 'png':\n",
    "        output = label.replace(' ','_') + '.png'\n",
    "        output = output.replace(',','')\n",
    "        if output_directory==None:\n",
    "            output_directory='png'\n",
    "        output = f\"{output_directory}/{output}\"\n",
    "    else:\n",
    "        output = 'inline'\n",
    "    \n",
    "    new_cases(df) # add a new_cases column to the dataframe\n",
    "    new_case_plot(df, label, days=14, centered=False, output=output)    \n",
    "    plt.close()\n",
    "    yellow_target(df, label, output=output)\n",
    "    plt.close()\n",
    "    trending(df, label, output=output)\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(all_df, popdf, query_type, query_state=None, query_region=None, query_county=None, output='inline', \n",
    "                output_directory=None, clip=False):\n",
    "    \"\"\" \n",
    "    Run pipeline on daily_reports data\n",
    "    \"\"\"\n",
    "    assert query_type in ['State', 'County', 'Region']\n",
    "    assert output in ['inline', 'png']\n",
    "\n",
    "    df, label = select_locality(all_df, popdf, query_type, query_state, query_region, query_county)\n",
    "\n",
    "    if clip:\n",
    "        df = clip_at_zero(df) # Start at the day before the first case\n",
    "    \n",
    "    return pipeline_helper(df, label, output, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to support time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def run_pipeline_series(all_sdf, popdf, query_type, query_state=None, query_region=None, query_county=None, output='inline', \n",
    "                output_directory=None, clip=False):\n",
    "    \"\"\"\n",
    "    Run pipeline on time_series data\n",
    "    \"\"\"\n",
    "    assert query_type in ['State', 'County', 'Region']\n",
    "    assert output in ['inline', 'png']\n",
    "\n",
    "    df = select_locality_series(all_sdf, popdf, query_type, query_state, query_region, query_county)\n",
    "    \n",
    "    if clip:\n",
    "        df = clip_at_zero(df) # Start at the day before the first case\n",
    "\n",
    "    label = df.Combined_Key.values[0]\n",
    "    return pipeline_helper(df, label, output, output_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def simplify_columns(df, date_cols=None):\n",
    "    if not date_cols:\n",
    "        # find which columns are dates\n",
    "        rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "        date_cols = [c for c in df.columns if rgx.search(c)]\n",
    "    #reorder = ['Province_State', 'Admin2', 'Country_Region', 'Combined_Key', 'Population', 'Region'] + date_cols\n",
    "    reorder = ['Admin2', 'Province_State', 'Country_Region', 'Combined_Key', 'Population'] + date_cols\n",
    "    df = df[reorder]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def merge_state_series(sdf, popdf, state=None):\n",
    "    merged = pd.DataFrame()\n",
    "\n",
    "    # verify there is only one state here --> if not, select it using the paramater\n",
    "    if len(set(sdf['Province_State'])) > 1:\n",
    "        sdf = get_state_data(state, sdf)\n",
    "    else: \n",
    "        state = sdf['Province_State'].values[0]\n",
    "        \n",
    "    # verify there is at least one row here\n",
    "    assert len(sdf) > 0\n",
    "\n",
    "    # find which columns are dates\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in sdf.columns if rgx.search(c)]\n",
    "\n",
    "    # Merge confirmed case totals\n",
    "    for date in date_cols:\n",
    "        merged[date] = sdf.groupby(sdf['Province_State'])[date].sum()\n",
    "    merged['Province_State'] = state\n",
    "    merged['Admin2'] = 'All'\n",
    "    merged['Country_Region'] = sdf['Country_Region'].values[0]\n",
    "    merged['Combined_Key'] = f'{state} State'\n",
    "    merged['Population'] = sdf.groupby(sdf['Province_State'])['Population'].sum()\n",
    "\n",
    "    merged = simplify_columns(merged, date_cols)\n",
    "\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def merge_region_series(sdf, popdf, region=None):\n",
    "    merged = pd.DataFrame()\n",
    "\n",
    "    # verify there is only one region here --> if not, select it using the paramater\n",
    "    if len(set(sdf['Region'])) > 1:\n",
    "        region_matches = sdf[(sdf.Region==region)]\n",
    "        region_matches.reset_index(drop=True, inplace=True)\n",
    "        sdf = pd.DataFrame(region_matches)\n",
    "    else:\n",
    "        region = sdf['Region'].values[0]\n",
    "        \n",
    "    state = sdf['Province_State'].values[0]\n",
    "        \n",
    "    # verify there is at least one row here\n",
    "    assert len(sdf) > 0\n",
    "\n",
    "    # find which columns are dates\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in sdf.columns if rgx.search(c)]\n",
    "\n",
    "    # Merge confirmed case totals\n",
    "    for date in date_cols:\n",
    "        merged[date] = sdf.groupby(sdf['Province_State'])[date].sum()\n",
    "    merged['Province_State'] = state\n",
    "    merged['Admin2'] = region\n",
    "    merged['Country_Region'] = sdf['Country_Region'].values[0]\n",
    "    merged['Combined_Key'] = f'{region} Region, {state}'\n",
    "    merged['Population'] = sdf.groupby(sdf['Province_State'])['Population'].sum()\n",
    "\n",
    "    merged = simplify_columns(merged, date_cols)\n",
    "    \n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def get_county_data_series(state, county, df):\n",
    "    merged = pd.DataFrame(df[(df.Province_State==state) & (df.Admin2==county)])\n",
    "    merged['Combined_Key'] = f'{county} County, {state}'\n",
    "    \n",
    "    merged = simplify_columns(merged)\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def transpose(sdf):\n",
    "    \"\"\" Convert the single-row time series JHU data to the table format \"\"\"\n",
    "    \n",
    "    # Assumes a single row\n",
    "    assert len(sdf) == 1\n",
    "    \n",
    "    # Save columns to a dictionary so we can retrieve later\n",
    "    keys = sdf.to_dict('records')[0]\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    non_date_cols = [c for c in sdf.columns if not rgx.search(c)]\n",
    "    sdf = sdf.drop(columns=non_date_cols)\n",
    "\n",
    "    # Transpose the data\n",
    "    df = sdf.transpose()\n",
    "\n",
    "    # Copy column 0 into Confirmed (otherwise reseting the index deletes this)\n",
    "    df['Confirmed'] = df[0]\n",
    "\n",
    "    # Create Last_Update column from the index and standardize dates to noon each day\n",
    "    df['Last_Update'] = df.index\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "    df.Last_Update = df.Last_Update.dt.strftime('%m/%d/%Y')\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "    \n",
    "    # Restore the non-date values into the columns\n",
    "    for col in non_date_cols:\n",
    "        df[col] = keys[col]\n",
    "    \n",
    "    # Reindex\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[['Last_Update', 'Confirmed'] + non_date_cols]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR time_series DATA ONLY\n",
    "def select_locality_series(sdf_all, popdf, query_type, query_state=None, query_region=None, query_county=None):\n",
    "    if 'Region' not in sdf_all:\n",
    "        annotate_regions(sdf_all, popdf)\n",
    "    if 'Population' not in sdf_all:\n",
    "        annotate_populations(sdf_all, popdf)\n",
    "    if query_type == 'State':\n",
    "        df = get_state_data(query_state, sdf_all) # OK\n",
    "        df = merge_state_series(df, popdf) # Rewritten\n",
    "    elif query_type == 'Region':\n",
    "        df = get_state_data(query_state, sdf_all) # OK\n",
    "        df = merge_region_series(df, popdf, region=query_region) # Rewritten\n",
    "    elif query_type == 'County':\n",
    "        df = get_county_data_series(query_state, query_county, sdf_all) # Rewritten\n",
    "\n",
    "    \n",
    "    return transpose(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support moving files around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate state graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATES = []\n"
     ]
    }
   ],
   "source": [
    "if 'STATEPLOT' in os.environ:\n",
    "    states = [os.environ['STATEPLOT']]\n",
    "else:\n",
    "    #states = ['Pennsylvania', 'Georgia', 'New York', 'Florida', 'New Jersey']\n",
    "    #states = ['Pennsylvania']\n",
    "    #states = ['Georgia', 'New York', 'Florida', 'New Jersey']\n",
    "    states = []\n",
    "print(f'STATES = {states}')\n",
    "statedir = 'states'\n",
    "tempdir = 'staging'\n",
    "\n",
    "# Make {tempdir} if it doesn't exist\n",
    "Path(tempdir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not series_data:\n",
    "    for state in states:\n",
    "        outdir = f'{tempdir}/{state}'.replace(' ','_')\n",
    "        # Make {outdir} if it doesn't exist\n",
    "        Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        counties = set(popdf[popdf.State==state].County)\n",
    "        \n",
    "        if state == 'New York': # remove NYC counties; JHU conflates into a single county\n",
    "            counties -= set(['Bronx', 'New York', 'Kings', 'Queens', 'Richmond', 'New York City'])\n",
    "\n",
    "        state_df = get_state_data(state, all_df)\n",
    "        state_df = clip_at_zero(state_df)\n",
    "\n",
    "        pbar = tqdm(sorted(counties))\n",
    "        for county in pbar:\n",
    "            pbar.set_description(f\"{state}:{county:20}\")\n",
    "            run_pipeline(state_df, popdf, query_type=\"County\", query_state=state, query_county=county, output=\"png\",\n",
    "                        output_directory=outdir)\n",
    "\n",
    "        # Annotate Regions\n",
    "        region_map = defaultdict(lambda: np.nan)\n",
    "        for d in popdf.to_dict('records'):\n",
    "            region_map[d['State'], d['County']] = d['Region']\n",
    "        state_df['Region'] = state_df[['Province_State', 'Admin2']].apply(lambda x: region_map[x[0], x[1]], axis=1)\n",
    "\n",
    "        regions = set(popdf.Region[(popdf.Region.notnull()) & (popdf.State==state)])\n",
    "\n",
    "        pbar = tqdm(sorted(regions))\n",
    "        for region in pbar:\n",
    "            pbar.set_description(f\"{state}:{region:20}\")\n",
    "            df = run_pipeline(state_df, popdf, query_type=\"Region\", query_state=state, query_region=region, output=\"png\",\n",
    "                         output_directory=outdir)\n",
    "\n",
    "        run_pipeline(state_df, popdf, query_type=\"State\", query_state=state, output='png',\n",
    "                     output_directory=outdir)\n",
    "\n",
    "    print(f\"All states completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movefiles(olddir, newdir, glob='*.png', chmod=None):\n",
    "    olddir = Path(olddir)\n",
    "    newdir = Path(newdir)\n",
    "    for oldsubdir in olddir.iterdir():\n",
    "        if oldsubdir.is_dir():\n",
    "            newsubdir = newdir.joinpath(oldsubdir.name)\n",
    "            # Be sure subdir exists in newdir\n",
    "            Path(newsubdir).mkdir(parents=True, exist_ok=True)\n",
    "            files=oldsubdir.glob(glob)\n",
    "\n",
    "            for file in files:\n",
    "                newpath = newsubdir.joinpath(file.name)\n",
    "                file.rename(newpath)\n",
    "                if chmod:\n",
    "                    newpath.chmod(chmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All states completed.\n"
     ]
    }
   ],
   "source": [
    "if series_data:\n",
    "    for state in states:\n",
    "        outdir = f'{tempdir}/{state}'.replace(' ','_')\n",
    "        # Make {outdir} if it doesn't exist\n",
    "        Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        counties = set(popdf[popdf.State==state].County)\n",
    "        \n",
    "        if state == 'New York': # remove NYC counties; JHU conflates into a single county\n",
    "            counties -= set(['Bronx', 'New York', 'Kings', 'Queens', 'Richmond', 'New York City'])\n",
    "        elif state in ['District of Columbia', 'Guam', 'Virgin Islands', 'Northern Mariana Islands']:\n",
    "            counties = set()\n",
    "            \n",
    "        state_df = get_state_data(state, all_sdf)\n",
    "        state_df = clip_at_zero_series(state_df)\n",
    "\n",
    "\n",
    "        # COUNTIES\n",
    "        pbar = tqdm(sorted(counties))\n",
    "        for county in pbar:\n",
    "            pbar.set_description(f\"{state}:{county:20}\")\n",
    "            pipedf = run_pipeline_series(state_df, popdf, query_type=\"County\", query_state=state, query_county=county, output=\"png\",\n",
    "                        output_directory=outdir)\n",
    "\n",
    "        # REGIONS\n",
    "        regions = set(popdf.Region[(popdf.Region.notnull()) & (popdf.State==state)])\n",
    "        pbar = tqdm(sorted(regions))\n",
    "        for region in pbar:\n",
    "            pbar.set_description(f\"{state}:{region:20}\")\n",
    "            df = run_pipeline_series(state_df, popdf, query_type=\"Region\", query_state=state, query_region=region, output=\"png\",\n",
    "                         output_directory=outdir)\n",
    "\n",
    "        # STATE\n",
    "        run_pipeline_series(state_df, popdf, query_type=\"State\", query_state=state, output='png',\n",
    "                     output_directory=outdir)\n",
    "\n",
    "    movefiles(tempdir, statedir, glob='*.png', chmod=0o644)\n",
    "    print(f\"All states completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-off graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup variables for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_off = False\n",
    "if one_off:\n",
    "    series_data = True\n",
    "    q_type='County'\n",
    "    q_state='Pennsylvania'\n",
    "    q_region='South East'\n",
    "    q_county='Montour'\n",
    "    output='inline'\n",
    "    output_dir='png'\n",
    "    if series_data:\n",
    "        df = run_pipeline_series(all_sdf, popdf, q_type, query_state=q_state, query_county=q_county, \n",
    "                                 query_region=q_region, output=output, output_directory=outdir)\n",
    "    else:\n",
    "        df = run_pipeline(all_df, popdf, q_type, query_state=q_state, query_county=q_county, \n",
    "                                 query_region=q_region, output=output, output_directory=outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct_df = get_covidtracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrevs = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', \n",
    "           'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'DC': 'District of Columbia', \n",
    "           'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', \n",
    "           'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', \n",
    "           'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', \n",
    "           'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', \n",
    "           'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', \n",
    "           'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', \n",
    "           'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', \n",
    "           'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', \n",
    "           'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'}\n",
    "rabbrevs = dict([(v,k) for (k,v) in abbrevs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covidtracking():\n",
    "    tracking_loc='covidtracking/states'\n",
    "    csv_file='daily.csv'\n",
    "    df = pd.read_csv(f'{tracking_loc}/{csv_file}')\n",
    "    df.date = pd.to_datetime(df.date, format='%Y%m%d')\n",
    "    return df\n",
    "    \n",
    "def filter_covidtracking(df, state):\n",
    "    state_df = pd.DataFrame(df[df.state==state].sort_values(by='date'))\n",
    "    state_df.reset_index(inplace=True)\n",
    "    return state_df\n",
    "    \n",
    "def augment_covidtracking(state_df, window=7):\n",
    "    \"\"\" \n",
    "    only works for a single state at a time\n",
    "    \"\"\"\n",
    "    state_df['positive'].fillna(0, inplace=True)\n",
    "    state_df['negative'].fillna(0, inplace=True)\n",
    "    state_df['pending'].fillna(0, inplace=True)\n",
    "    \n",
    "\n",
    "    # cumulative\n",
    "    state_df['positive_rate'] = state_df.positive / (state_df.positive + state_df.negative)\n",
    "    state_df['daily_positive'] = state_df.positive.subtract(state_df.positive.shift(1), fill_value=0)\n",
    "    state_df['daily_negative'] = state_df.negative.subtract(state_df.negative.shift(1), fill_value=0)\n",
    "    \n",
    "    # {window}-day daily test rate\n",
    "    dp = f'daily_positive_{window}'\n",
    "    dn = f'daily_negative_{window}'\n",
    "    dpr= f'daily_positive_rate_{window}'\n",
    "    state_df[dp] = state_df.daily_positive.rolling(window=window, min_periods=1, center=False).sum()\n",
    "    state_df[dn] = state_df.daily_negative.rolling(window=window, min_periods=1, center=False).sum()\n",
    "    state_df[dpr]= state_df[dp]/(state_df[dp] + state_df[dn])\n",
    "    \n",
    "    # {window}-day daily number of tests average\n",
    "    state_df['tests'] = state_df.positive + state_df.negative\n",
    "    state_df['new_tests'] = state_df.tests.subtract(state_df.tests.shift(1), fill_value=0)\n",
    "    nt = f'new_tests_{window}'\n",
    "    state_df[nt] = state_df.new_tests.rolling(window=window, min_periods=1, center=False).mean()\n",
    "    return state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_test_rate(df, label, window=7, mindate=\"2020-04-01\", output=None):\n",
    "    \"\"\"\n",
    "    run on covidtracking data\n",
    "    \"\"\"    \n",
    "    \n",
    "    if mindate is not None:\n",
    "        df = df[df.date > mindate]\n",
    "        \n",
    "    dpr = f'daily_positive_rate_{window}'\n",
    "    g = sns.lineplot(df['date'], df[dpr], label=f\"positive test rate: {window} day average\")\n",
    "    #sns.lineplot(df['date'], df['positive_rate'], label=\"cumulative positive test rate\", ax=g)\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Positive test rate\", title=f\"Positive test rate over time\\n{label}\")\n",
    "\n",
    "    ymax = max(0.5, max(df.daily_positive_rate_7))\n",
    "    g.set_ylim(0, ymax)\n",
    "\n",
    "    leg = g.legend(loc='best', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    #else:\n",
    "    #    output = output.replace(\"'\",\"\").replace('.png', '_yellow_target.png')\n",
    "    #    plt.savefig(output, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptr_plus(df, label, window=7, mindate=\"2020-04-01\", output=None):\n",
    "    dpr = f'daily_positive_rate_{window}'\n",
    "    nt = f'new_tests_{window}'\n",
    "\n",
    "    if mindate is not None:\n",
    "        df = df[df.date > mindate]\n",
    "    \n",
    "    formatted_dates = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    g=sns.barplot(formatted_dates, df[nt], label=\"number of tests\", color='green')\n",
    "    t = g.twinx()\n",
    "    \n",
    "    sns.lineplot(np.arange(len(df)), df[dpr], color=\"black\", label=\"positive test rate\", ax=t)\n",
    "    #slopes = np.where(df['trend_14'].isnull(), 0, df['slope_14'])\n",
    "    #sns.lineplot(np.arange(len(df)), slopes, color=\"black\", label=\"14-day slope\", ax=t)\n",
    "\n",
    "    labels = limit_xticks(g.get_xticklabels())\n",
    "    g.set_xticklabels(labels,rotation=90)\n",
    "\n",
    "    #g.set_ylim(-14,14)\n",
    "    \n",
    "    title=f\"Number of tests and positive test rate: {window}-day average\\n{label}\"\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Number of tests\", title=title)\n",
    "    t.set(ylabel=\"Positive test rate\")\n",
    "\n",
    "    ymax = max(0.5, max(df.daily_positive_rate_7))\n",
    "    t.set_ylim(0, ymax)\n",
    "    \n",
    "    leg = t.legend(loc='best', frameon=False)\n",
    "\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_trend.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = 'Oregon'\n",
    "#state_df = filter_covidtracking(ct_df, rabbrevs[state])\n",
    "#augment_covidtracking(state_df)\n",
    "#ptr_plus(state_df, state, output='inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('nlp': virtualenv)",
   "language": "python",
   "name": "python37764bitnlpvirtualenv16bd9fbbc17243058c9cd2c35a0e7820"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
