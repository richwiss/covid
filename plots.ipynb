{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires:\n",
    "\n",
    "$ npm install -g electron@6.1.4 orca  (omit -g on cs machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-time directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = '.'\n",
    "population_loc = f'{base_loc}/resources'\n",
    "\n",
    "# jhu_loc is the root directory of the JHU data repository\n",
    "jhu_loc = f'{base_loc}/COVID-19'\n",
    "csse_loc = f'{jhu_loc}/csse_covid_19_data/csse_covid_19_daily_reports'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load population and region data\n",
    "\n",
    "Note: Regions only supported for Pennsylvania and New York in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_population_data():\n",
    "    \"\"\" load population and region data for counties in PA and other supported states \"\"\"\n",
    "    df = pd.read_csv(f'{population_loc}/county-populations.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jhu_population_data():\n",
    "    \"\"\" load population and region data for counties in PA and other supported states \"\"\"\n",
    "    df = pd.read_csv(f'{jhu_loc}/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thinking about dealing with Utah non-county data\n",
    "if False:\n",
    "    cdf = load_population_data()\n",
    "    jdf = load_jhu_population_data()\n",
    "\n",
    "    seen = set()\n",
    "    for (i, row) in cdf.iterrows():\n",
    "        (county, state) = row['County'], row['State']\n",
    "        key = f'{county}_{state}'\n",
    "        seen.add(key)\n",
    "\n",
    "    seen2= set()\n",
    "    for i, row in jdf.iterrows():\n",
    "        if row['Country_Region']=='US':\n",
    "            if pd.isnull(row.Admin2) or row.Admin2.startswith('Out of') or row.Admin2 == 'Unassigned':\n",
    "                continue\n",
    "            (county, state) = row['Admin2'], row['Province_State']\n",
    "            key = f'{county}_{state}'\n",
    "            seen2.add(key)\n",
    "\n",
    "    seen2-seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JHU data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_data():\n",
    "    \"\"\" read the series data in the JHU directory \"\"\"\n",
    "    series_loc = f'{jhu_loc}/csse_covid_19_data/csse_covid_19_time_series'\n",
    "    df = pd.read_csv(f'{series_loc}/time_series_covid19_confirmed_US.csv', dtype={\"FIPS\": str})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and filter all data for just one state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_state(df, state, popdf):\n",
    "    \"\"\" merge individual counties in a state into a single row \"\"\"\n",
    "    statepop_dict = state_populations(popdf)\n",
    "\n",
    "    merged = pd.DataFrame()\n",
    "    merged['Last_Update'] = df[df.Province_State==state].groupby(df.Last_Update)['Last_Update'].unique()    \n",
    "    merged['Admin2'] = 'All'\n",
    "    merged['Province_State'] = state\n",
    "    merged['Country_Region'] = df.Country_Region.unique()[0]\n",
    "\n",
    "    merged['Deaths'] = df[df.Province_State==state].groupby(df.Last_Update)['Deaths'].sum()\n",
    "    merged['Confirmed'] = df[df.Province_State==state].groupby(df.Last_Update)['Confirmed'].sum()\n",
    "    merged['Recovered'] = df[df.Province_State==state].groupby(df.Last_Update)['Recovered'].sum()\n",
    "    merged['Active'] = df[df.Province_State==state].groupby(df.Last_Update)['Active'].sum()\n",
    "    merged['Population'] = statepop_dict[state]\n",
    "\n",
    "    merged['Last_Update'] = merged.index\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_data(state, df):\n",
    "    state_matches = df[(df.Province_State==state)]\n",
    "    state_matches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return pd.DataFrame(state_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge for just one region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_region(df, region, popdf):\n",
    "    \"\"\" for PA, merge the data into regions \"\"\"\n",
    "    regionpop_dict = region_populations(popdf)\n",
    "    \n",
    "    merged = pd.DataFrame()\n",
    "    merged['Last_Update'] = df[df.Region==region].groupby(df.Last_Update)['Last_Update'].unique()    \n",
    "    merged['Admin2'] = region\n",
    "    merged['Province_State'] = df.Province_State.unique()[0]\n",
    "    merged['Country_Region'] = df.Country_Region.unique()[0]\n",
    "\n",
    "    merged['Deaths'] = df[df.Region==region].groupby(df.Last_Update)['Deaths'].sum()\n",
    "    merged['Confirmed'] = df[df.Region==region].groupby(df.Last_Update)['Confirmed'].sum()\n",
    "    merged['Recovered'] = df[df.Region==region].groupby(df.Last_Update)['Recovered'].sum()\n",
    "    merged['Active'] = df[df.Region==region].groupby(df.Last_Update)['Active'].sum()\n",
    "\n",
    "    merged['Population'] = regionpop_dict[region]\n",
    "    \n",
    "    merged['Last_Update'] = merged.index\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter all data for just one county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_data(state, county, df):\n",
    "    county_matches = df[(df.Province_State==state) & (df.Admin2==county)]\n",
    "    county_matches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return pd.DataFrame(county_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the dataframe with region information, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def annotate_regions(df, popdf):\n",
    "    \"\"\" For counties in Pennsylvania, annotate the dataframe with the region \"\"\"\n",
    "    \n",
    "    def annotator(row):\n",
    "        poprow = popdf[(popdf.State==row.Province_State)&(popdf.County==row.Admin2)]\n",
    "        if len(poprow) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return poprow.Region.values[0]\n",
    "\n",
    "    df['Region'] = df.apply(annotator, axis=1)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_regions(df, popdf):\n",
    "    \"\"\" Annotate the dataframe with the region, if available (PA and NYC only) \"\"\"\n",
    "\n",
    "    region_map = defaultdict(lambda: np.nan)\n",
    "    for d in popdf.to_dict('records'):\n",
    "        region_map[d['State'], d['County']] = d['Region']\n",
    "    df['Region'] = df[['Province_State', 'Admin2']].apply(lambda x: region_map[x[0], x[1]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate the dataframe with populations, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_populations(df, popdf):\n",
    "    \"\"\" Annotate the dataframe with populations, if available \"\"\"\n",
    "        \n",
    "    pop_map = defaultdict(lambda: np.nan)\n",
    "    for d in popdf.to_dict('records'):\n",
    "        pop_map[d['State'], d['County']] = d['Population']\n",
    "    df['Population'] = df.loc[:,['Province_State','Admin2']].apply(lambda x: pop_map[x[0], x[1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_populations(popdf):\n",
    "    return dict(popdf.groupby(popdf.State)['Population'].sum().items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_populations(popdf):\n",
    "    \"\"\" for PA, calculate the population of each region \"\"\"   \n",
    "    return dict(popdf.groupby(popdf.Region)['Population'].sum().items())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the appropriate locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_locality(all_df, popdf, query_type, query_state=None, query_region=None, query_county=None):\n",
    "    if query_type == 'State':\n",
    "        label = f'{query_state} State'\n",
    "        df = get_state_data(query_state, all_df)\n",
    "        df = merge_state(df, query_state, popdf)\n",
    "    elif query_type == 'Region':\n",
    "        label = f'{query_region} Region, {query_state}'\n",
    "        df = get_state_data(query_state, all_df)\n",
    "        if 'Region' in df.columns:\n",
    "            df['Admin2'] = df['Region']\n",
    "        else:\n",
    "            annotate_regions(df, popdf)\n",
    "        df = merge_region(df, query_region, popdf)\n",
    "    elif query_type == 'County':\n",
    "        label = f'{query_county} County, {query_state}'\n",
    "        df=get_county_data(query_state, query_county, all_df)\n",
    "        annotate_populations(df, popdf)\n",
    "    \n",
    "    return df, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute daily and average new cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cases(df):\n",
    "    \"\"\" given a DataFrame with a .Confirmed field, add a .New_Cases field that\n",
    "    has new cases per day. \"\"\"\n",
    "    df['New_Cases'] = df.Confirmed.subtract(df.Confirmed.shift(1), fill_value=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_new_cases(df, days, centered=False):\n",
    "    \"\"\" this computes day the trailing average in the final day \"\"\"\n",
    "    \"\"\" compute the moving average over {days} days and add as day_avg_{days} to the df \"\"\"\n",
    "    field = f'day_avg_{days}'\n",
    "    df[field] = df.New_Cases.rolling(window=14, min_periods=1, center=centered).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_avg(dates):\n",
    "  refdate = datetime.datetime(2019, 1, 1)\n",
    "  return refdate + sum([date - refdate for date in dates], datetime.timedelta()) / len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily new cases and 14-day moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_case_plot(df, label, days=14, centered=False, output=None):\n",
    "\n",
    "    if centered:\n",
    "        date_field = f'Centered_Date_{days}'\n",
    "    else:\n",
    "        date_field='Last_Update'\n",
    "    average_new_cases(df, days, centered=centered)\n",
    "\n",
    "        \n",
    "    g = sns.lineplot(df['Last_Update'], df['New_Cases'], label=\"Daily new cases\")\n",
    "    sns.lineplot(df[date_field], df[f'day_avg_{days}'], ax=g, label=f\"{days} day moving average\")\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"New Cases\", title=f\"New Cases Per Day\\n{label}\")\n",
    "    leg = g.legend(loc='upper left', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_new_cases.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow target: 50 new cases over 14 days per 100K people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newcase_sum(df, days, perpop=1):\n",
    "    \"\"\" \n",
    "    compute the sum of {days} days and {days}_sum to the df \n",
    "    if perpop is not 1, calculate the same weighted by the population pop\n",
    "    \"\"\"\n",
    "    field = f'sum_{days}'\n",
    "    df[field] = df.New_Cases.rolling(window=days, min_periods=1).sum()\n",
    "    df[field] *= perpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellow_target(df, label, output=None):\n",
    "    population = set(df.Population).pop()\n",
    "    newcase_sum(df, 14, perpop=100000/population)\n",
    "    target = 50\n",
    "    \n",
    "    g = sns.lineplot(df['Last_Update'], df['sum_14'], label=\"14 day caseload per 100K\")\n",
    "    sns.lineplot(df['Last_Update'], [target]*len(df), label=\"Yellow Target\", ax=g)\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"14 days cases per 100K\", title=f\"Progress towards yellow target\\n{label}\")\n",
    "    leg = g.legend(loc='lower right', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_yellow_target.png')\n",
    "        plt.savefig(output, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days trending downward in 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_xticks(labels, num=5):\n",
    "    \"\"\"\n",
    "    For some reason I can't limit the number of xticks so here I'm\n",
    "    just doing it myself by erasing the text of the xticks I don't want\n",
    "    \"\"\"\n",
    "        \n",
    "    target_ticks = set([0, len(labels)-1])\n",
    "    for i in range(1, num-1):\n",
    "        pos= int(round(len(labels)/(num-1)*i,0))\n",
    "        target_ticks.add(pos)\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        if i not in target_ticks:\n",
    "            labels[i].set_text(\"\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(period):\n",
    "    if len(period) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        m, b = np.polyfit(np.arange(len(period)), period, 1)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend(df, days):\n",
    "    \"\"\" \n",
    "    compute the trendline for the past {days} days as slope_{days} and\n",
    "    the number of days within those {days} that the trend is worsening \n",
    "    (positive) or improving (negative) as {days}_trend\n",
    "    \"\"\"\n",
    "    slopes = []\n",
    "    trends = []\n",
    "\n",
    "    # Get the slope of the trend line for the past {days} days.\n",
    "    sfield=f'slope_{days}'\n",
    "    df[sfield] = df.New_Cases.rolling(window=days, min_periods=1).apply(fit)\n",
    "\n",
    "    # Get the number of times the slope was positive in last {days} days.\n",
    "    field = f'trend_{days}'\n",
    "    df[field] = df[sfield].rolling(window=14, min_periods=14).apply(lambda x: (x>0).sum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending(df, label, days=14, output=None):\n",
    "    df = trend(df, 14)\n",
    "\n",
    "    tfield = f'trend_{days}'\n",
    "    sfield = f'slope_{days}'\n",
    "    \n",
    "    formatted_dates = df['Last_Update'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    g=sns.barplot(formatted_dates, df[tfield], label=\"increasing trends\", color='red')\n",
    "    sns.barplot(formatted_dates, df[tfield]-14, label=\"decreasing trends\", color='green')\n",
    "    t = g.twinx()\n",
    "    \n",
    "    sns.lineplot(np.arange(len(df)), df[sfield], color=\"black\", label=\"14-day slope\", ax=t)\n",
    "    #slopes = np.where(df['trend_14'].isnull(), 0, df['slope_14'])\n",
    "    #sns.lineplot(np.arange(len(df)), slopes, color=\"black\", label=\"14-day slope\", ax=t)\n",
    "\n",
    "    labels = limit_xticks(g.get_xticklabels())\n",
    "    g.set_xticklabels(labels,rotation=90)\n",
    "\n",
    "    g.set_ylim(-14,14)\n",
    "    title=f\"Number of days in the past two weeks with a positive or negative trend\\n{label}\"\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Number of days\", title=title)\n",
    "    t.set(ylabel=\"slope of 14-day trend\")\n",
    "    slope_lim = max(abs(df[df[sfield].notna()][sfield]))*1.1\n",
    "    t.set_ylim(-slope_lim,slope_lim)\n",
    "    leg = t.legend(loc='lower left', frameon=False)\n",
    "\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_trend.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues\n",
    "\n",
    "* States allocate cases to \"Unassigned\" if county is unknown\n",
    "* \"Out of CO\", \"Out of GA\", \"Out of MI\", \"Out of OK\", \"Out of TN\" is listed as a county\n",
    "* Dukes, MA and Nantucket, MA -> \"Dukes and Nantucket\"\n",
    "* Federal Correctional Institution (FCI), MI; Michigan Department of Corrections (MDOC), MI\n",
    "* Kansas City, MO reported as a standalone county when it actually appears in multiple counties\n",
    "* New York City, NY is reported but counties are Richmond, Queens, New York, Kings and Bronx\n",
    "* Counties in Utah don't align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read county population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "popdf = load_population_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JHU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sdf = get_series_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output all graphs for specified state, region or county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_at_date(df, date):\n",
    "    \"\"\"\n",
    "    Start the time series on this date\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_at_zero_series(df):\n",
    "    \"\"\"\n",
    "    Start the data the day before the first confirmed case\n",
    "    \"\"\"\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in df.columns if rgx.search(c)]\n",
    "    drops = []\n",
    "    for c in date_cols:\n",
    "        sm = df[c].sum()\n",
    "        if sm == 0:\n",
    "            drops.append(c)\n",
    "        elif sm > 0:\n",
    "            break\n",
    "    if len(drops) < len(df.columns):\n",
    "        df = df.drop(columns=drops)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_helper(df, label, output, output_directory, plotly=False):\n",
    "    df = df.sort_values(by='Last_Update', ignore_index=True)\n",
    "    if output == 'png':\n",
    "        output = label.replace(' ','_') + '.png'\n",
    "        output = output.replace(',','')\n",
    "        if output_directory==None:\n",
    "            output_directory='png'\n",
    "        output = f\"{output_directory}/{output}\"\n",
    "    else:\n",
    "        output = 'inline'\n",
    "    \n",
    "    new_cases(df) # add a new_cases column to the dataframe\n",
    "    if plotly:\n",
    "        pngScale=0.25\n",
    "        new_case_plotly(df, label, days=14, centered=False, output=output, pngScale=pngScale)    \n",
    "        yellow_target_plotly(df, label, output=output, pngScale=pngScale)\n",
    "        trending_plotly(df, label, output=output, pngScale=pngScale)\n",
    "    else:\n",
    "        new_case_plot(df, label, days=14, centered=False, output=output)    \n",
    "        plt.close()\n",
    "        yellow_target(df, label, output=output)\n",
    "        plt.close()\n",
    "        trending(df, label, output=output)\n",
    "        plt.close()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotly graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_color = {'blue': '#1f77b4', 'orange': '#ff7f0e', 'green': '#2ca02c', 'purple': '#9467bd', 'olive': '#bcbd22'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_figure_plotly(fig, output, title, description, pngScale=None):\n",
    "    full_layout = go.Layout(\n",
    "        title=title,\n",
    "        showlegend=True,\n",
    "    )\n",
    "    if output == 'inline':\n",
    "        fig.update_layout(full_layout)\n",
    "        fig.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', f'_{description}.png')\n",
    "        fig.write_image(output, scale=pngScale)\n",
    "        fig.update_layout(full_layout)\n",
    "        output = output.replace('.png', '.html')\n",
    "        #fig.write_html(output, include_plotlyjs=\"plotly.min.js\", full_html=True)\n",
    "        fig.write_html(output, include_plotlyjs=False, full_html=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_case_plotly(df, label, days=14, centered=False, output=None, pngScale=None):\n",
    "\n",
    "    # add average newcases in 'day_avg_{days}' column\n",
    "    average_new_cases(df, days, centered=centered)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace( # daily new cases\n",
    "        go.Scatter(\n",
    "            x = df.Last_Update,\n",
    "            y = df.New_Cases,\n",
    "            mode = 'lines+markers',\n",
    "            name = 'Daily new cases',\n",
    "            line_color = plt_color['olive'],\n",
    "            hovertemplate = '<b>%{y}</b>',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace( # moving average\n",
    "        go.Scatter(\n",
    "            x = df.Last_Update,\n",
    "            y = df[f'day_avg_{days}'],\n",
    "            mode = 'lines',\n",
    "            name = f'{days} day moving average',\n",
    "            line_color = plt_color['blue'],\n",
    "            hovertemplate = '<b>%{y:.1f}</b>',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Positive tests\",\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"#7f7f7f\"\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    fig.update_layout(layout)\n",
    "    \n",
    "    title=f\"New cases per day: {label}\"\n",
    "    write_figure_plotly(fig, output, title, 'new_cases', pngScale=pngScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_plotly(df, label, output=None, pngScale=None):\n",
    "    days = 14\n",
    "    df = trend(df, days)\n",
    "\n",
    "    tfield = f'trend_{days}'\n",
    "    sfield = f'slope_{days}'\n",
    "\n",
    "    formatted_dates = df['Last_Update'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "    uptrend = df['trend_14']\n",
    "    downtrend=df['trend_14']-14\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = df.Last_Update,\n",
    "            y = uptrend,\n",
    "            name = f'Days with a positive trend',\n",
    "            marker_color = 'red',\n",
    "            marker_line_width=1,\n",
    "            offset=0,\n",
    "            hovertemplate = '<b>%{y}</b>',\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = df.Last_Update,\n",
    "            y = downtrend,\n",
    "            name = f'Days with a negative trend',\n",
    "            marker_color = 'green',\n",
    "            marker_line_width=1,\n",
    "            offset=0,\n",
    "            text = -downtrend,\n",
    "            hovertemplate = '<b>%{text}</b>',\n",
    "        ),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    slope_range = max(abs(df[sfield]))*1.05\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df.Last_Update,\n",
    "            y = df[sfield],\n",
    "            name = 'Slope of two-week trend',\n",
    "            line_color='black',\n",
    "            hovertemplate = '<b>%{y:.2f}</b>',\n",
    "        ),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        showlegend=False,  # updated for inline and html\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=f\"Number of days\",\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"#7f7f7f\"\n",
    "        ),\n",
    "        hovermode=\"x unified\",\n",
    "        yaxis = {'range': [-14, 14], 'dtick':7},\n",
    "        yaxis2= {'range': [-slope_range, slope_range], 'showgrid': False}\n",
    "    )\n",
    "\n",
    "    fig.update_layout(layout)\n",
    "    \n",
    "    title=f\"Two week trends: {label}\"\n",
    "    write_figure_plotly(fig, output, title, 'trend', pngScale=pngScale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yellow_target_plotly(df, label, output=None, pngScale=None):\n",
    "    population = set(df.Population).pop()\n",
    "\n",
    "    days = 14\n",
    "    target = 50\n",
    "\n",
    "    percap = df[f'day_avg_{days}']*100000 / population * 14\n",
    "    target_lst = [target] * len(df.Last_Update)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = df.Last_Update,\n",
    "                y = percap,\n",
    "                mode = 'lines+markers',\n",
    "                name = f'{days} day per 100K',\n",
    "                line_color = plt_color['blue'],\n",
    "                hovertemplate = '<b>%{y:.1f}</b>',\n",
    "            )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = df.Last_Update,\n",
    "            y = target_lst,\n",
    "            mode = 'lines',\n",
    "            name = f'{target} cases per 100K',\n",
    "            line_color = plt_color['orange'],\n",
    "            hoverinfo = \"none\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        showlegend=False,  # updated for inline and html\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=f\"Positive tests\",\n",
    "        font=dict(\n",
    "            size=12,\n",
    "            color=\"#7f7f7f\"\n",
    "        ),\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    fig.update_layout(layout)\n",
    "    title = f\"New cases over 14 days per 100K residents: {label}\"\n",
    "    write_figure_plotly(fig, output, title, 'yellow_target', pngScale=pngScale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to support time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_series(all_sdf, popdf, query_type, query_state=None, query_region=None, query_county=None, output='inline', \n",
    "                output_directory=None, clip=False, plotly=False):\n",
    "    \"\"\"\n",
    "    Run pipeline on time_series data\n",
    "    \"\"\"\n",
    "    assert query_type in ['State', 'County', 'Region']\n",
    "    assert output in ['inline', 'png']\n",
    "\n",
    "    df = select_locality_series(all_sdf, popdf, query_type, query_state, query_region, query_county)\n",
    "    \n",
    "    if clip:\n",
    "        df = clip_at_zero(df) # Start at the day before the first case\n",
    "\n",
    "    label = df.Combined_Key.values[0]\n",
    "    return pipeline_helper(df, label, output, output_directory, plotly=plotly)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_columns(df, date_cols=None):\n",
    "    if not date_cols:\n",
    "        # find which columns are dates\n",
    "        rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "        date_cols = [c for c in df.columns if rgx.search(c)]\n",
    "    #reorder = ['Province_State', 'Admin2', 'Country_Region', 'Combined_Key', 'Population', 'Region'] + date_cols\n",
    "    reorder = ['Admin2', 'Province_State', 'Country_Region', 'Combined_Key', 'Population'] + date_cols\n",
    "    df = df[reorder]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_state_series(sdf, popdf, state=None):\n",
    "    merged = pd.DataFrame()\n",
    "\n",
    "    # verify there is only one state here --> if not, select it using the paramater\n",
    "    if len(set(sdf['Province_State'])) > 1:\n",
    "        sdf = get_state_data(state, sdf)\n",
    "    else: \n",
    "        state = sdf['Province_State'].values[0]\n",
    "        \n",
    "    # verify there is at least one row here\n",
    "    assert len(sdf) > 0\n",
    "\n",
    "    # find which columns are dates\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in sdf.columns if rgx.search(c)]\n",
    "\n",
    "    # Merge confirmed case totals\n",
    "    for date in date_cols:\n",
    "        merged[date] = sdf.groupby(sdf['Province_State'])[date].sum()\n",
    "    merged['Province_State'] = state\n",
    "    merged['Admin2'] = 'All'\n",
    "    merged['Country_Region'] = sdf['Country_Region'].values[0]\n",
    "    merged['Combined_Key'] = f'{state} State'\n",
    "    merged['Population'] = sdf.groupby(sdf['Province_State'])['Population'].sum()\n",
    "\n",
    "    merged = simplify_columns(merged, date_cols)\n",
    "\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_region_series(sdf, popdf, region=None):\n",
    "    merged = pd.DataFrame()\n",
    "\n",
    "    # verify there is only one region here --> if not, select it using the paramater\n",
    "    if len(set(sdf['Region'])) > 1:\n",
    "        region_matches = sdf[(sdf.Region==region)]\n",
    "        region_matches.reset_index(drop=True, inplace=True)\n",
    "        sdf = pd.DataFrame(region_matches)\n",
    "    else:\n",
    "        region = sdf['Region'].values[0]\n",
    "        \n",
    "    state = sdf['Province_State'].values[0]\n",
    "        \n",
    "    # verify there is at least one row here\n",
    "    assert len(sdf) > 0\n",
    "\n",
    "    # find which columns are dates\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    date_cols = [c for c in sdf.columns if rgx.search(c)]\n",
    "\n",
    "    # Merge confirmed case totals\n",
    "    for date in date_cols:\n",
    "        merged[date] = sdf.groupby(sdf['Province_State'])[date].sum()\n",
    "    merged['Province_State'] = state\n",
    "    merged['Admin2'] = region\n",
    "    merged['Country_Region'] = sdf['Country_Region'].values[0]\n",
    "    merged['Combined_Key'] = f'{region} Region, {state}'\n",
    "    merged['Population'] = sdf.groupby(sdf['Province_State'])['Population'].sum()\n",
    "\n",
    "    merged = simplify_columns(merged, date_cols)\n",
    "    \n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_data_series(state, county, df):\n",
    "    merged = pd.DataFrame(df[(df.Province_State==state) & (df.Admin2==county)])\n",
    "    merged['Combined_Key'] = f'{county} County, {state}'\n",
    "    \n",
    "    merged = simplify_columns(merged)\n",
    "    merged.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(sdf):\n",
    "    \"\"\" Convert the single-row time series JHU data to the table format \"\"\"\n",
    "    \n",
    "    # Assumes a single row\n",
    "    assert len(sdf) == 1\n",
    "    \n",
    "    # Save columns to a dictionary so we can retrieve later\n",
    "    keys = sdf.to_dict('records')[0]\n",
    "    rgx = re.compile(r'\\d+/\\d+/\\d+')\n",
    "    non_date_cols = [c for c in sdf.columns if not rgx.search(c)]\n",
    "    sdf = sdf.drop(columns=non_date_cols)\n",
    "\n",
    "    # Transpose the data\n",
    "    df = sdf.transpose()\n",
    "\n",
    "    # Copy column 0 into Confirmed (otherwise reseting the index deletes this)\n",
    "    df['Confirmed'] = df[0]\n",
    "\n",
    "    # Create Last_Update column from the index and standardize dates to noon each day\n",
    "    df['Last_Update'] = df.index\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "    df.Last_Update = df.Last_Update.dt.strftime('%m/%d/%Y')\n",
    "    df.Last_Update = pd.to_datetime(df.Last_Update)\n",
    "    \n",
    "    # Restore the non-date values into the columns\n",
    "    for col in non_date_cols:\n",
    "        df[col] = keys[col]\n",
    "    \n",
    "    # Reindex\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[['Last_Update', 'Confirmed'] + non_date_cols]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_locality_series(sdf_all, popdf, query_type, query_state=None, query_region=None, query_county=None):\n",
    "    if 'Region' not in sdf_all:\n",
    "        annotate_regions(sdf_all, popdf)\n",
    "    if 'Population' not in sdf_all:\n",
    "        annotate_populations(sdf_all, popdf)\n",
    "    if query_type == 'State':\n",
    "        df = get_state_data(query_state, sdf_all) # OK\n",
    "        df = merge_state_series(df, popdf) # Rewritten\n",
    "    elif query_type == 'Region':\n",
    "        df = get_state_data(query_state, sdf_all) # OK\n",
    "        df = merge_region_series(df, popdf, region=query_region) # Rewritten\n",
    "    elif query_type == 'County':\n",
    "        df = get_county_data_series(query_state, query_county, sdf_all) # Rewritten\n",
    "\n",
    "    \n",
    "    return transpose(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate state graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardw/Desktop/covid/states / /Users/richardw/Desktop/covid/staging\n"
     ]
    }
   ],
   "source": [
    "if 'STATEPLOT' in os.environ:\n",
    "    states = [os.environ['STATEPLOT']]\n",
    "else:\n",
    "    #states = ['Pennsylvania', 'Georgia', 'New York', 'Florida', 'New Jersey']\n",
    "    states = ['Pennsylvania']\n",
    "\n",
    "check_modification_time = False\n",
    "if states == ['ALL']:\n",
    "    states = ['Pennsylvania', 'Florida', 'Georgia', 'New Jersey', 'New York', 'California', 'North Carolina', \n",
    "              'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Colorado', 'Connecticut', 'Delaware', \n",
    "              'District of Columbia', 'Guam', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', \n",
    "              'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', \n",
    "              'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Mexico', \n",
    "              'North Dakota', 'Northern Mariana Islands', 'Ohio', 'Oklahoma', 'Oregon', 'Rhode Island', \n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', \n",
    "              'Virgin Islands', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "    check_modification_time = True\n",
    "\n",
    "coviddir = os.environ.get('COVIDDIR', None)\n",
    "if not coviddir:\n",
    "    home = os.environ.get('HOME', '.')  #fall back to cwd if HOME undefined\n",
    "    coviddir = f'{home}/Desktop/covid'\n",
    "statedir = f'{coviddir}/states'\n",
    "tempdir = f'{coviddir}/staging'\n",
    "print(f'{statedir} / {tempdir}')\n",
    "\n",
    "# Make {tempdir} if it doesn't exist\n",
    "Path(tempdir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movefiles(olddir, newdir, glob='*.png', chmod=None):\n",
    "    olddir = Path(olddir)\n",
    "    newdir = Path(newdir)\n",
    "    for oldsubdir in olddir.iterdir():\n",
    "        if oldsubdir.is_dir():\n",
    "            newsubdir = newdir.joinpath(oldsubdir.name)\n",
    "            # Be sure subdir exists in newdir\n",
    "            Path(newsubdir).mkdir(parents=True, exist_ok=True)\n",
    "            files=oldsubdir.glob(glob)\n",
    "\n",
    "            for file in files:\n",
    "                newpath = newsubdir.joinpath(file.name)\n",
    "                file.rename(newpath)\n",
    "                if chmod:\n",
    "                    newpath.chmod(chmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pennsylvania:York                : 100%|██████████| 67/67 [00:19<00:00,  3.45it/s]\n",
      "Pennsylvania:South West          : 100%|██████████| 6/6 [00:02<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving staged files.\n"
     ]
    }
   ],
   "source": [
    "plotly=True\n",
    "generated=[]\n",
    "\n",
    "for state in states:\n",
    "    ustate = state.replace(' ','_')\n",
    "    \n",
    "    if check_modification_time:\n",
    "            state_index = pathlib.Path(f'{statedir}/{ustate}/index.php')\n",
    "            if state_index.exists():\n",
    "                state_index_mtime = state_index.stat().st_mtime\n",
    "                series_loc = f'{jhu_loc}/csse_covid_19_data/csse_covid_19_time_series'\n",
    "                csv_path = pathlib.Path(f'{series_loc}/time_series_covid19_confirmed_US.csv')\n",
    "                csv_path_mtime = csv_path.stat().st_mtime\n",
    "                if csv_path_mtime < state_index_mtime:\n",
    "                    continue\n",
    "        \n",
    "    generated.append(state)\n",
    "    outpath = pathlib.Path(f'{tempdir}/{ustate}')\n",
    "    outpath.mkdir(parents=True, exist_ok=True)  # mkdir if it doesn't exist\n",
    "\n",
    "    counties = set(popdf[popdf.State==state].County)\n",
    "\n",
    "    if state == 'New York': # remove NYC counties; JHU conflates into a single county\n",
    "        counties -= set(['Bronx', 'New York', 'Kings', 'Queens', 'Richmond', 'New York City'])\n",
    "    elif state in ['District of Columbia', 'Guam', 'Virgin Islands', 'Northern Mariana Islands']:\n",
    "        counties = set()\n",
    "\n",
    "    state_df = get_state_data(state, all_sdf)\n",
    "    state_df = clip_at_zero_series(state_df)\n",
    "\n",
    "\n",
    "    # COUNTIES\n",
    "    pbar = tqdm(sorted(counties))\n",
    "    for county in pbar:\n",
    "        pbar.set_description(f\"{state}:{county:20}\")\n",
    "        pipedf = run_pipeline_series(state_df, popdf, query_type=\"County\", query_state=state, query_county=county, \n",
    "                                     output=\"png\", output_directory=outpath, plotly=plotly)\n",
    "\n",
    "    # REGIONS\n",
    "    regions = set(popdf.Region[(popdf.Region.notnull()) & (popdf.State==state)])\n",
    "    pbar = tqdm(sorted(regions))\n",
    "    for region in pbar:\n",
    "        pbar.set_description(f\"{state}:{region:20}\")\n",
    "        df = run_pipeline_series(state_df, popdf, query_type=\"Region\", query_state=state, query_region=region, \n",
    "                                 output=\"png\", output_directory=outpath, plotly=plotly)\n",
    "\n",
    "    # STATE\n",
    "    run_pipeline_series(state_df, popdf, query_type=\"State\", query_state=state, output='png',\n",
    "                        output_directory=outpath, plotly=plotly)\n",
    "\n",
    "if len(generated) > 0:\n",
    "    print(f'Moving staged files.')\n",
    "    movefiles(tempdir, statedir, glob='*.png', chmod=0o644)\n",
    "    movefiles(tempdir, statedir, glob='*.html', chmod=0o644)\n",
    "else:\n",
    "    print(f'Files up to date.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "movefiles(tempdir, statedir, glob='*.html', chmod=0o644)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-off graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup variables for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_off = False\n",
    "if one_off:\n",
    "    q_type='County'\n",
    "    q_state='Pennsylvania'\n",
    "    q_region='South East'\n",
    "    q_county='Delaware'\n",
    "    output='inline'\n",
    "    outdir='png'\n",
    "    plotly=True\n",
    "    df = run_pipeline_series(all_sdf, popdf, q_type, query_state=q_state, query_county=q_county, \n",
    "                             query_region=q_region, output=output, output_directory=outdir, plotly=plotly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct_df = get_covidtracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrevs = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', \n",
    "           'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'DC': 'District of Columbia', \n",
    "           'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', \n",
    "           'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', \n",
    "           'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', \n",
    "           'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', \n",
    "           'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', \n",
    "           'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', \n",
    "           'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', \n",
    "           'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', \n",
    "           'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'}\n",
    "rabbrevs = dict([(v,k) for (k,v) in abbrevs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covidtracking():\n",
    "    tracking_loc='covidtracking/states'\n",
    "    csv_file='daily.csv'\n",
    "    df = pd.read_csv(f'{tracking_loc}/{csv_file}')\n",
    "    df.date = pd.to_datetime(df.date, format='%Y%m%d')\n",
    "    return df\n",
    "    \n",
    "def filter_covidtracking(df, state):\n",
    "    state_df = pd.DataFrame(df[df.state==state].sort_values(by='date'))\n",
    "    state_df.reset_index(inplace=True)\n",
    "    return state_df\n",
    "    \n",
    "def augment_covidtracking(state_df, window=7):\n",
    "    \"\"\" \n",
    "    only works for a single state at a time\n",
    "    \"\"\"\n",
    "    state_df['positive'].fillna(0, inplace=True)\n",
    "    state_df['negative'].fillna(0, inplace=True)\n",
    "    state_df['pending'].fillna(0, inplace=True)\n",
    "    \n",
    "\n",
    "    # cumulative\n",
    "    state_df['positive_rate'] = state_df.positive / (state_df.positive + state_df.negative)\n",
    "    state_df['daily_positive'] = state_df.positive.subtract(state_df.positive.shift(1), fill_value=0)\n",
    "    state_df['daily_negative'] = state_df.negative.subtract(state_df.negative.shift(1), fill_value=0)\n",
    "    \n",
    "    # {window}-day daily test rate\n",
    "    dp = f'daily_positive_{window}'\n",
    "    dn = f'daily_negative_{window}'\n",
    "    dpr= f'daily_positive_rate_{window}'\n",
    "    state_df[dp] = state_df.daily_positive.rolling(window=window, min_periods=1, center=False).sum()\n",
    "    state_df[dn] = state_df.daily_negative.rolling(window=window, min_periods=1, center=False).sum()\n",
    "    state_df[dpr]= state_df[dp]/(state_df[dp] + state_df[dn])\n",
    "    \n",
    "    # {window}-day daily number of tests average\n",
    "    state_df['tests'] = state_df.positive + state_df.negative\n",
    "    state_df['new_tests'] = state_df.tests.subtract(state_df.tests.shift(1), fill_value=0)\n",
    "    nt = f'new_tests_{window}'\n",
    "    state_df[nt] = state_df.new_tests.rolling(window=window, min_periods=1, center=False).mean()\n",
    "    return state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_test_rate(df, label, window=7, mindate=\"2020-04-01\", output=None):\n",
    "    \"\"\"\n",
    "    run on covidtracking data\n",
    "    \"\"\"    \n",
    "    \n",
    "    if mindate is not None:\n",
    "        df = df[df.date > mindate]\n",
    "        \n",
    "    dpr = f'daily_positive_rate_{window}'\n",
    "    g = sns.lineplot(df['date'], df[dpr], label=f\"positive test rate: {window} day average\")\n",
    "    #sns.lineplot(df['date'], df['positive_rate'], label=\"cumulative positive test rate\", ax=g)\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Positive test rate\", title=f\"Positive test rate over time\\n{label}\")\n",
    "\n",
    "    ymax = max(0.5, max(df.daily_positive_rate_7))\n",
    "    g.set_ylim(0, ymax)\n",
    "\n",
    "    leg = g.legend(loc='best', frameon=False)\n",
    "    plt.xticks(rotation=90)\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    #else:\n",
    "    #    output = output.replace(\"'\",\"\").replace('.png', '_yellow_target.png')\n",
    "    #    plt.savefig(output, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptr_plus(df, label, window=7, mindate=\"2020-04-01\", output=None):\n",
    "    dpr = f'daily_positive_rate_{window}'\n",
    "    nt = f'new_tests_{window}'\n",
    "\n",
    "    if mindate is not None:\n",
    "        df = df[df.date > mindate]\n",
    "    \n",
    "    formatted_dates = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    g=sns.barplot(formatted_dates, df[nt], label=\"number of tests\", color='green')\n",
    "    t = g.twinx()\n",
    "    \n",
    "    sns.lineplot(np.arange(len(df)), df[dpr], color=\"black\", label=\"positive test rate\", ax=t)\n",
    "    #slopes = np.where(df['trend_14'].isnull(), 0, df['slope_14'])\n",
    "    #sns.lineplot(np.arange(len(df)), slopes, color=\"black\", label=\"14-day slope\", ax=t)\n",
    "\n",
    "    labels = limit_xticks(g.get_xticklabels())\n",
    "    g.set_xticklabels(labels,rotation=90)\n",
    "\n",
    "    #g.set_ylim(-14,14)\n",
    "    \n",
    "    title=f\"Number of tests and positive test rate: {window}-day average\\n{label}\"\n",
    "    g.set(xlabel=\"\\nDate\", ylabel=\"Number of tests\", title=title)\n",
    "    t.set(ylabel=\"Positive test rate\")\n",
    "\n",
    "    ymax = max(0.5, max(df.daily_positive_rate_7))\n",
    "    t.set_ylim(0, ymax)\n",
    "    \n",
    "    leg = t.legend(loc='best', frameon=False)\n",
    "\n",
    "    if output == 'inline':\n",
    "        plt.show()\n",
    "    else:\n",
    "        output = output.replace(\"'\",\"\").replace('.png', '_trend.png')\n",
    "        plt.savefig(output, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = 'Oregon'\n",
    "#state_df = filter_covidtracking(ct_df, rabbrevs[state])\n",
    "#augment_covidtracking(state_df)\n",
    "#ptr_plus(state_df, state, output='inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('nlp': virtualenv)",
   "language": "python",
   "name": "python37764bitnlpvirtualenv16bd9fbbc17243058c9cd2c35a0e7820"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
